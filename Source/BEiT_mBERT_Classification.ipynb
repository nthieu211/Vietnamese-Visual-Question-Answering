{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ti2q34oPIewJ","outputId":"1cc2f6f9-0d8b-43f2-bf47-6cbf790a2b00","executionInfo":{"status":"ok","timestamp":1704123239503,"user_tz":-420,"elapsed":20406,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.0/468.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q yacs unicodedata2 nltk transformers sentencepiece accelerate transformers[torch]"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_kaVxKd9dT6j","outputId":"a9212d02-fccb-416b-9914-0c10b577c8ef","executionInfo":{"status":"ok","timestamp":1704123258169,"user_tz":-420,"elapsed":18681,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading...\n","From: https://drive.google.com/uc?id=1TztHmmwN8R6MGEABPnxgaBErZ4vDOjdJ\n","To: /content/vivqa-dataset.zip\n","100% 527M/527M [00:04<00:00, 112MB/s]\n"]}],"source":["!gdown 1TztHmmwN8R6MGEABPnxgaBErZ4vDOjdJ\n","!unzip -q 'vivqa-dataset.zip'\n","!rm /content/vivqa-dataset.zip"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uvq5SpGoMKap","outputId":"7b66d4d7-c468-4b23-a541-64a866920b52","executionInfo":{"status":"ok","timestamp":1704123276436,"user_tz":-420,"elapsed":18294,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2eCn_4HndbGr","executionInfo":{"status":"ok","timestamp":1704123291752,"user_tz":-420,"elapsed":15320,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["import os\n","import re\n","import string\n","import unicodedata\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import shutil\n","from tqdm import tqdm\n","import itertools\n","from PIL import Image\n","from collections import Counter\n","from typing import Dict, List, Union\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import LambdaLR\n","from torch.nn import NLLLoss\n","\n","from transformers import BeitImageProcessor, BeitModel\n","from transformers import BertTokenizer\n","from transformers.models.bert.modeling_bert import (\n","    BertConfig,\n","    BertEmbeddings,\n","    BertEncoder,\n","    BertPreTrainedModel,\n",")"]},{"cell_type":"markdown","metadata":{"id":"O7ZPofYXrfcx"},"source":["# Prepare Dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GudoE6sEdW4q","executionInfo":{"status":"ok","timestamp":1704123291752,"user_tz":-420,"elapsed":9,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["DATASET_DIR = 'vivqa-dataset'\n","IMAGES_DIR = os.path.join(DATASET_DIR, 'images')\n","TRAIN_PATH = os.path.join(DATASET_DIR, 'train.csv')\n","TEST_PATH = os.path.join(DATASET_DIR, 'test.csv')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yMzQSxXfeXuR","executionInfo":{"status":"ok","timestamp":1704123291752,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["# Load train.csv\n","train_df = pd.read_csv(TRAIN_PATH)\n","if train_df.shape[1] > 4:\n","    train_df = train_df.iloc[: , 1:] # Drop first column (index)\n","\n","# Load test.csv\n","test_df = pd.read_csv(TEST_PATH)\n","if test_df.shape[1] > 4:\n","    test_df = test_df.iloc[: , 1:] # Drop first column (index)"]},{"cell_type":"markdown","metadata":{"id":"7EDE_MwclkRX"},"source":["## Remove duplicate"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_XnLNvzloK0","outputId":"0257d073-9611-41aa-ba94-c97e73982145","executionInfo":{"status":"ok","timestamp":1704123291753,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (11819, 4)\n","Test shape: (2987, 4)\n"]}],"source":["# Remove duplicate train_df\n","train_df.drop_duplicates(keep=False, inplace=True)\n","train_df.to_csv(TRAIN_PATH, index=False)\n","print(f'Train shape: {train_df.shape}')\n","\n","# Remove duplicate test_df\n","test_df.drop_duplicates(keep=False, inplace=True)\n","test_df.to_csv(TEST_PATH, index=False)\n","print(f'Test shape: {test_df.shape}')"]},{"cell_type":"markdown","metadata":{"id":"SWCtfCp1chs5"},"source":["# Config"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mowwbb_Pclpt","outputId":"c7d6865a-4eed-454e-a72a-8c614c901b9b","executionInfo":{"status":"ok","timestamp":1704123292238,"user_tz":-420,"elapsed":491,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["beit_mbert_classification\n"]}],"source":["from yacs.config import CfgNode\n","import yaml\n","\n","if os.path.exists('/content/drive'):\n","    BASE_DIR = '/content/drive/MyDrive'\n","else:\n","    BASE_DIR = ''\n","\n","config_file = os.path.join(BASE_DIR, \"ViVQA-Models\", \"beit_mbert_classification.yaml\")\n","\n","with open(config_file, \"r\") as stream:\n","    try:\n","        CONFIG =  CfgNode(init_dict=yaml.safe_load(stream))\n","    except yaml.YAMLError as exc:\n","        print(exc)\n","\n","print(CONFIG.MODEL.NAME)"]},{"cell_type":"markdown","metadata":{"id":"FBiskLU1lorL"},"source":["# Build dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"mUxwmLANDLCC","executionInfo":{"status":"ok","timestamp":1704123292238,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["class ClassificationVocab(object):\n","    # This class is especially designed for ViVQA dataset by treating the VQA as a classification task.\n","    # For more information, please visit https://arxiv.org/abs/1708.02711\n","\n","    def __init__(self, config):\n","\n","        self.tokenizer = config.TOKENIZER\n","\n","        self.padding_token = config.PAD_TOKEN\n","        self.bos_token = config.BOS_TOKEN\n","        self.eos_token = config.EOS_TOKEN\n","        self.unk_token = config.UNK_TOKEN\n","\n","        self.make_vocab([\n","            config.DF_PATH.TRAIN,\n","            config.DF_PATH.TEST\n","        ])\n","\n","        counter = self.freqs.copy()\n","\n","        min_freq = max(config.MIN_FREQ, 1)\n","\n","        specials = [self.padding_token, self.bos_token, self.eos_token, self.unk_token]\n","        itos = specials\n","        # frequencies of special tokens are not counted when building vocabulary\n","        # in frequency order\n","        for tok in specials:\n","            del counter[tok]\n","\n","        # sort by frequency, then alphabetically\n","        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n","        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n","\n","        for word, freq in words_and_frequencies:\n","            if freq < min_freq:\n","                break\n","            itos.append(word)\n","\n","        self.itos = {i: tok for i, tok in enumerate(itos)}\n","        self.stoi = {tok: i for i, tok in enumerate(itos)}\n","\n","        self.specials = [self.padding_token, self.bos_token, self.eos_token, self.unk_token]\n","\n","        self.padding_idx = self.stoi[self.padding_token]\n","        self.bos_idx = self.stoi[self.bos_token]\n","        self.eos_idx = self.stoi[self.eos_token]\n","        self.unk_idx = self.stoi[self.unk_token]\n","\n","\n","    def make_vocab(self, df_path_list):\n","        self.freqs = Counter()\n","        itoa = set()\n","        self.max_question_length = 0\n","        for df_path in df_path_list:\n","            df = pd.read_csv(df_path)\n","            for question in df['question']:\n","                question = preprocess_sentence(question, self.tokenizer)\n","                self.freqs.update(question)\n","                if len(question) + 2 > self.max_question_length:\n","                    self.max_question_length = len(question) + 2\n","            for answer in df['answer']:\n","                answer = \" \".join(preprocess_sentence(answer, self.tokenizer))\n","                itoa.add(answer)\n","\n","        self.itoa = {ith: answer for ith, answer in enumerate(itoa)}\n","        self.atoi = {answer: ith for ith, answer in self.itoa.items()}\n","        self.total_answers = len(self.atoi)\n","\n","    def encode_question(self, question: List[str]) -> torch.Tensor:\n","        \"\"\" Turn a question into a vector of indices and a question length \"\"\"\n","        vec = torch.ones(self.max_question_length).long() * self.padding_idx\n","        for i, token in enumerate([self.bos_token] + question + [self.eos_token]):\n","            vec[i] = self.stoi[token] if token in self.stoi else self.unk_idx\n","        return vec\n","\n","    def encode_answer(self, answer: List[str]) -> torch.Tensor:\n","        answer = \" \".join(answer)\n","        return torch.tensor([self.atoi[answer]], dtype=torch.long)\n","\n","    def decode_question(self, question_vecs: torch.Tensor, join_words=True) -> List[str]:\n","        '''\n","            question_vecs: (bs, max_length)\n","        '''\n","        questions = []\n","        for vec in question_vecs:\n","            question = \" \".join([self.itos[idx] for idx in vec.tolist() if self.itos[idx] not in self.specials])\n","            if join_words:\n","                questions.append(question)\n","            else:\n","                questions.append(question.strip().split())\n","        return questions\n","\n","    def decode_answer(self, answer_vecs: torch.Tensor, join_word=False) -> Union[List[str], List[List[str]]]:\n","        answers = []\n","        list_answers = answer_vecs.tolist()\n","        for answer_idx in list_answers:\n","            ans_i = self.itoa[answer_idx] if join_word else self.itoa[answer_idx].split()\n","            answers.append(str(ans_i))\n","        return answers"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"cZVZT4vbiuJe","executionInfo":{"status":"ok","timestamp":1704123292238,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["# Build dataset class\n","class ViVQA_Dataset(torch.utils.data.Dataset):\n","  \"\"\"\n","  Dataset class for the ViVQA dataset.\n","  \"\"\"\n","  def __init__(self, df, img_dir, vocab):\n","    self.df = df\n","    self.img_dir = img_dir\n","    self.vocab = vocab\n","\n","  def __len__(self):\n","    return self.df.shape[0]\n","\n","  def __getitem__(self, idx):\n","    question = self.df.loc[idx, 'question']\n","    answer = self.df.loc[idx, 'answer']\n","    image_id = self.df.loc[idx, 'img_id']\n","    quest_type = self.df.loc[idx, 'type']\n","\n","    img_file = os.path.join(self.img_dir, f'image_{image_id}.jpg')\n","\n","    return {'image':img_file, 'question':question, 'answer':answer}"]},{"cell_type":"markdown","metadata":{"id":"DuwJAzxaC0X3"},"source":["# Metrics"]},{"cell_type":"markdown","metadata":{"id":"k4k64XQEC0X3"},"source":["## CIDEr"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"RrxK82_hC0X3","executionInfo":{"status":"ok","timestamp":1704123292238,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["import copy\n","from collections import defaultdict\n","import math\n","\n","def precook(s, n=4):\n","    \"\"\"\n","    Takes a string as input and returns an object that can be given to\n","    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n","    can take string arguments as well.\n","    :param s: string : sentence to be converted into ngrams\n","    :param n: int    : number of ngrams for which representation is calculated\n","    :return: term frequency vector for occuring ngrams\n","    \"\"\"\n","    words = str(s).split()\n","    counts = defaultdict(int)\n","    for k in range(1,n+1):\n","        for i in range(len(words)-k+1):\n","            ngram = tuple(words[i:i+k])\n","            counts[ngram] += 1\n","    return counts\n","\n","def cook_refs(refs, n=4): ## lhuang: oracle will call with \"average\"\n","    '''Takes a list of reference sentences for a single segment\n","    and returns an object that encapsulates everything that BLEU\n","    needs to know about them.\n","    :param refs: list of string : reference sentences for some image\n","    :param n: int : number of ngrams for which (ngram) representation is calculated\n","    :return: result (list of dict)\n","    '''\n","    return [precook(ref, n) for ref in refs]\n","\n","def cook_test(test, n=4):\n","    '''Takes a test sentence and returns an object that\n","    encapsulates everything that BLEU needs to know about it.\n","    :param test: list of string : hypothesis sentence for some image\n","    :param n: int : number of ngrams for which (ngram) representation is calculated\n","    :return: result (dict)\n","    '''\n","    return precook(test, n)\n","\n","class CiderScorer(object):\n","    \"\"\"CIDEr scorer.\n","    \"\"\"\n","\n","    def __init__(self, refs, test=None, n=4, sigma=6.0, doc_frequency=None, ref_len=None):\n","        ''' singular instance '''\n","        self.n = n\n","        self.sigma = sigma\n","        self.crefs = []\n","        self.ctest = []\n","        self.doc_frequency = defaultdict(float)\n","        self.ref_len = None\n","\n","        for k in range(len(refs)):\n","            self.crefs.append(cook_refs(refs[k]))\n","            if test is not None:\n","                self.ctest.append(cook_test(test[k][0]))  ## N.B.: -1\n","            else:\n","                self.ctest.append(None)  # lens of crefs and ctest have to match\n","\n","        if doc_frequency is None and ref_len is None:\n","            # compute idf\n","            self.compute_doc_freq()\n","            # compute log reference length\n","            self.ref_len = np.log(float(len(self.crefs)))\n","        else:\n","            self.doc_frequency = doc_frequency\n","            self.ref_len = ref_len\n","\n","    def compute_doc_freq(self):\n","        '''\n","        Compute term frequency for reference data.\n","        This will be used to compute idf (inverse document frequency later)\n","        The term frequency is stored in the object\n","        :return: None\n","        '''\n","        for refs in self.crefs:\n","            # refs, k ref captions of one image\n","            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n","                self.doc_frequency[ngram] += 1\n","            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n","\n","    def compute_cider(self):\n","        def counts2vec(cnts):\n","            \"\"\"\n","            Function maps counts of ngram to vector of tfidf weights.\n","            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n","            The n-th entry of array denotes length of n-grams.\n","            :param cnts:\n","            :return: vec (array of dict), norm (array of float), length (int)\n","            \"\"\"\n","            vec = [defaultdict(float) for _ in range(self.n)]\n","            length = 0\n","            norm = [0.0 for _ in range(self.n)]\n","            for (ngram,term_freq) in cnts.items():\n","                # give word count 1 if it doesn't appear in reference corpus\n","                df = np.log(max(1.0, self.doc_frequency[ngram]))\n","                # ngram index\n","                n = len(ngram)-1\n","                # tf (term_freq) * idf (precomputed idf) for n-grams\n","                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n","                # compute norm for the vector.  the norm will be used for computing similarity\n","                norm[n] += pow(vec[n][ngram], 2)\n","\n","                if n == 1:\n","                    length += term_freq\n","            norm = [np.sqrt(n) for n in norm]\n","            return vec, norm, length\n","\n","        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n","            '''\n","            Compute the cosine similarity of two vectors.\n","            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n","            :param vec_ref: array of dictionary for vector corresponding to reference\n","            :param norm_hyp: array of float for vector corresponding to hypothesis\n","            :param norm_ref: array of float for vector corresponding to reference\n","            :param length_hyp: int containing length of hypothesis\n","            :param length_ref: int containing length of reference\n","            :return: array of score for each n-grams cosine similarity\n","            '''\n","            delta = float(length_hyp - length_ref)\n","            # measure consine similarity\n","            val = np.array([0.0 for _ in range(self.n)])\n","            for n in range(self.n):\n","                # ngram\n","                for (ngram,count) in vec_hyp[n].items():\n","                    # vrama91 : added clipping\n","                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n","\n","                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n","                    val[n] /= (norm_hyp[n]*norm_ref[n])\n","\n","                assert(not math.isnan(val[n]))\n","                # vrama91: added a length based gaussian penalty\n","                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n","            return val\n","\n","        scores = []\n","        for test, refs in zip(self.ctest, self.crefs):\n","            # compute vector for test captions\n","            vec, norm, length = counts2vec(test)\n","            # compute vector for ref captions\n","            score = np.array([0.0 for _ in range(self.n)])\n","            for ref in refs:\n","                vec_ref, norm_ref, length_ref = counts2vec(ref)\n","                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n","            # change by vrama91 - mean of ngram scores, instead of sum\n","            score_avg = np.mean(score)\n","            # divide by number of references\n","            score_avg /= len(refs)\n","            # multiply score by 10\n","            score_avg *= 10.0\n","            # append score of an image to the score list\n","            scores.append(score_avg)\n","        return scores\n","\n","    def compute_score(self):\n","        # compute cider score\n","        score = self.compute_cider()\n","        # debug\n","        # print score\n","        return np.mean(np.array(score)), np.array(score)"]},{"cell_type":"markdown","metadata":{"id":"-lj_4auvC0X4"},"source":["## Extract match"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"rQGzKVxpC0X4","executionInfo":{"status":"ok","timestamp":1704123292239,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["class Exact_Match:\n","    def compute_score(self, y_true, y_pred):\n","        if y_true==y_pred:\n","            return 1\n","        else:\n","            return 0"]},{"cell_type":"markdown","metadata":{"id":"CgJsda56C0X4"},"source":["## F1 Score"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"LgoSZQshC0X4","executionInfo":{"status":"ok","timestamp":1704123292239,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["class F1:\n","  def Precision(self,y_true,y_pred):\n","    if y_pred is None:\n","       return 0\n","    common = set(y_true) & set(y_pred)\n","    return len(common) / len(set(y_pred))\n","\n","  def Recall(self,y_true,y_pred):\n","    common = set(y_true) & set(y_pred)\n","    return len(common) / len(set(y_true))\n","\n","  def compute_score(self,y_true,y_pred):\n","    if len(y_pred) == 0 or len(y_true) == 0:\n","        return int(y_pred == y_true)\n","\n","    precision = self.Precision(y_true, y_pred)\n","    recall = self.Recall(y_true, y_pred)\n","\n","    if precision == 0 or recall == 0:\n","        return 0\n","    f1 = 2*precision*recall / (precision+recall)\n","    return f1"]},{"cell_type":"markdown","metadata":{"id":"jB8bXph0C0X4"},"source":["## Wup"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5oZ1YZCC0X5","outputId":"c641c3c9-06f6-42fe-837a-33bc6d63ee42","executionInfo":{"status":"ok","timestamp":1704123292993,"user_tz":-420,"elapsed":759,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]}],"source":["import nltk\n","nltk.download('wordnet')\n","from nltk.corpus import wordnet\n","\n","class Wup:\n","    def get_semantic_field(self,a):\n","        weight = 1.0\n","        semantic_field = wordnet.synsets(str(a), pos=wordnet.NOUN)\n","        return (semantic_field,weight)\n","\n","    def get_stem_word(self,a):\n","        \"\"\"\n","        Sometimes answer has form word\\d+:wordid.\n","        If so we return word and downweight\n","        \"\"\"\n","        weight = 1.0\n","        return (a,weight)\n","    def compute_score(self, a: str, b: str, similarity_threshold: float = 0.9):\n","        \"\"\"\n","        Returns Wu-Palmer similarity score.\n","        More specifically, it computes:\n","            max_{x \\in interp(a)} max_{y \\in interp(b)} wup(x,y)\n","            where interp is a 'interpretation field'\n","        \"\"\"\n","        global_weight=1.0\n","\n","        (a,global_weight_a)=self.get_stem_word(a)\n","        (b,global_weight_b)=self.get_stem_word(b)\n","        global_weight = min(global_weight_a,global_weight_b)\n","\n","        if a==b:\n","            # they are the same\n","            return 1.0*global_weight\n","\n","        if a==[] or b==[]:\n","            return 0\n","\n","        interp_a,weight_a = self.get_semantic_field(a)\n","        interp_b,weight_b = self.get_semantic_field(b)\n","\n","        if interp_a == [] or interp_b == []:\n","            return 0\n","\n","        # we take the most optimistic interpretation\n","        global_max=0.0\n","        for x in interp_a:\n","            for y in interp_b:\n","                local_score=x.wup_similarity(y)\n","                if local_score > global_max:\n","                    global_max=local_score\n","\n","        # we need to use the semantic fields and therefore we downweight\n","        # unless the score is high which indicates both are synonyms\n","        if global_max < similarity_threshold:\n","            interp_weight = 0.1\n","        else:\n","            interp_weight = 1.0\n","\n","        final_score=global_max*weight_a*weight_b*interp_weight*global_weight\n","        return final_score"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MKbHQ5FTC0X5","executionInfo":{"status":"ok","timestamp":1704123292994,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["import re\n","import unicodedata\n","\n","def normalize_text(text):\n","    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n","    text = text.lower().strip()\n","    return text\n","\n","def preprocess_sentence(sentence: str, tokenizer=None):\n","    sentence = sentence.lower()\n","    sentence = unicodedata.normalize('NFC', sentence)\n","    sentence = re.sub(r\"[“”]\", \"\\\"\", sentence)\n","    sentence = re.sub(r\"!\", \" ! \", sentence)\n","    sentence = re.sub(r\"\\?\", \" ? \", sentence)\n","    sentence = re.sub(r\":\", \" : \", sentence)\n","    sentence = re.sub(r\";\", \" ; \", sentence)\n","    sentence = re.sub(r\",\", \" , \", sentence)\n","    sentence = re.sub(r\"\\\"\", \" \\\" \", sentence)\n","    sentence = re.sub(r\"'\", \" ' \", sentence)\n","    sentence = re.sub(r\"\\(\", \" ( \", sentence)\n","    sentence = re.sub(r\"\\[\", \" [ \", sentence)\n","    sentence = re.sub(r\"\\)\", \" ) \", sentence)\n","    sentence = re.sub(r\"\\]\", \" ] \", sentence)\n","    sentence = re.sub(r\"/\", \" / \", sentence)\n","    sentence = re.sub(r\"\\.\", \" . \", sentence)\n","    sentence = re.sub(r\"-\", \" - \", sentence)\n","    sentence = re.sub(r\"\\$\", \" $ \", sentence)\n","    sentence = re.sub(r\"\\&\", \" & \", sentence)\n","    sentence = re.sub(r\"\\*\", \" * \", sentence)\n","    # tokenize the sentence\n","    if tokenizer is None:\n","        tokenizer = lambda s: s\n","    sentence = tokenizer(sentence)\n","    sentence = \" \".join(sentence.strip().split()) # remove duplicated spaces\n","    tokens = sentence.strip().split()\n","\n","    return tokens\n","\n","class ScoreCalculator:\n","    def __init__(self):\n","        self.f1_caculate=F1()\n","        self.em_caculate=Exact_Match()\n","        self.Wup_caculate=Wup()\n","    #F1 score character level\n","    def f1_char(self,labels: List[str], preds: List[str]) -> float:\n","        scores=[]\n","        for i in range(len(labels)):\n","            scores.append(self.f1_caculate.compute_score(str(preprocess_sentence(normalize_text(labels[i]))).split(),str(preprocess_sentence(normalize_text(preds[i]))).split()))\n","        return np.mean(scores)\n","\n","    #F1 score token level\n","    def f1_token(self,labels: List[str], preds: List[str]) -> float:\n","        scores=[]\n","        for i in range(len(labels)):\n","            scores.append(self.f1_caculate.compute_score(str(preprocess_sentence(normalize_text(labels[i]))).split(),str(preprocess_sentence(normalize_text(preds[i]))).split()))\n","        return np.mean(scores)\n","    #Excat match score\n","    def em(self,labels: List[str], preds: List[str]) -> float:\n","        scores=[]\n","        for i in range(len(labels)):\n","            scores.append(self.em_caculate.compute_score(str(preprocess_sentence(normalize_text(labels[i]))).split(),str(preprocess_sentence(normalize_text(preds[i]))).split()))\n","        return np.mean(scores)\n","    #Wup score\n","    def wup(self,labels: List[str], preds: List[str]) -> float:\n","        scores=[]\n","        for i in range(len(labels)):\n","            scores.append(self.Wup_caculate.compute_score(str(preprocess_sentence(normalize_text(labels[i]))).split(),str(preprocess_sentence(normalize_text(preds[i]))).split()))\n","        return np.mean(scores)\n","    #Cider score\n","    def cider_score(self,labels: List[str], preds: List[str]) -> float:\n","        labels=[[preprocess_sentence(normalize_text(label))] for label in labels]\n","        preds=[[preprocess_sentence(normalize_text(pred))] for pred in preds ]\n","        cider_caculate= CiderScorer(labels, test=preds, n=4, sigma=6.)\n","        scores,_=cider_caculate.compute_score()\n","        return scores"]},{"cell_type":"markdown","metadata":{"id":"rZTiCWMorZwl"},"source":["# Model"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"XbV9ipS5aP_D","executionInfo":{"status":"ok","timestamp":1704123292994,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["def generate_padding_mask(sequences, padding_idx: int) -> torch.BoolTensor:\n","    '''\n","        sequences: (bs, seq_len, dim)\n","    '''\n","    if sequences is None:\n","        return None\n","\n","    if len(sequences.shape) == 2: # (bs, seq_len)\n","        __seq = sequences.unsqueeze(dim=-1) # (bs, seq_len, 1)\n","    else:\n","        __seq = sequences\n","\n","    mask = (torch.sum(__seq, dim=-1) == (padding_idx*__seq.shape[-1])).long() * -10e4 # (b_s, seq_len)\n","    return mask.unsqueeze(1).unsqueeze(1) # (bs, 1, 1, seq_len)\n","\n","class TextBert(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.embeddings = BertEmbeddings(config)\n","        self.encoder = BertEncoder(config)\n","        self.init_weights()\n","\n","    def forward(self, txt_inds, txt_mask):\n","        encoder_inputs = self.embeddings(txt_inds)\n","\n","        attention_mask = txt_mask\n","        head_mask = [None] * self.config.num_hidden_layers\n","        encoder_outputs = self.encoder(\n","            encoder_inputs, attention_mask, head_mask=head_mask\n","        )\n","        seq_output = encoder_outputs[0]\n","\n","        return seq_output\n","\n","class BertEmbedding(nn.Module):\n","    def __init__(self, config, vocab):\n","        super().__init__()\n","\n","        self.device = config.DEVICE\n","\n","        bert_config = BertConfig.from_pretrained(config.PRETRAINED_NAME)\n","\n","        self.tokenizer = BertTokenizer.from_pretrained(config.PRETRAINED_NAME)\n","        self.embedding = TextBert(bert_config)\n","        self.embedding = self.embedding.from_pretrained(config.PRETRAINED_NAME)\n","\n","        # freeze all parameters of pretrained model\n","        for param in self.embedding.parameters():\n","            param.requires_grad = False\n","\n","        self.proj = nn.Linear(config.D_PRETRAINED_FEATURE, config.D_MODEL)\n","        self.gelu = nn.GELU()\n","        self.dropout = nn.Dropout(config.DROPOUT)\n","\n","    def forward(self, questions: List[str]):\n","        inputs = self.tokenizer(questions, return_tensors=\"pt\", padding=True).input_ids.to(self.device)\n","        padding_mask = generate_padding_mask(inputs, padding_idx=self.tokenizer.pad_token_id)\n","        features = self.embedding(inputs, padding_mask)\n","\n","        out = self.proj(features)\n","        out = self.dropout(self.gelu(out))\n","\n","        return out, padding_mask"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"G2X9xa1TedZh","executionInfo":{"status":"ok","timestamp":1704123292994,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["class BEiTEmbedding(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        self.device = torch.device(config.DEVICE)\n","\n","        self.feature_extractor = BeitImageProcessor.from_pretrained(config.PRETRAINED_NAME)\n","        self.backbone = BeitModel.from_pretrained(config.PRETRAINED_NAME)\n","        # freeze all parameters of pretrained model\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","\n","        self.proj = nn.Linear(config.D_PRETRAINED_FEATURE, config.D_MODEL)\n","        self.gelu = nn.GELU()\n","        self.dropout = nn.Dropout(config.DROPOUT)\n","\n","    def forward(self, images: List[Image.Image]):\n","        inputs = self.feature_extractor(images, return_tensors=\"pt\").to(self.device)\n","        features = self.backbone(**inputs).last_hidden_state\n","        padding_mask = generate_padding_mask(features, padding_idx=0)\n","\n","        out = self.proj(features)\n","        out = self.dropout(self.gelu(out))\n","\n","        return out, padding_mask"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"GbiSGSo8eiH_","executionInfo":{"status":"ok","timestamp":1704123292994,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["class BEiTmBERTClassification(nn.Module):\n","    def __init__(self, config, vocab):\n","        super().__init__()\n","        self.d_model = config.D_MODEL\n","\n","        self.text_embedding = BertEmbedding(config.TEXT_EMBEDDING, vocab)\n","        self.vision_encoder = BEiTEmbedding(config.VISION_EMBEDDING)\n","\n","        self.fusion = nn.Linear(config.D_MODEL, config.D_MODEL)\n","        self.dropout = nn.Dropout(config.DROPOUT)\n","        self.norm = nn.LayerNorm(config.D_MODEL)\n","\n","        self.proj = nn.Linear(config.D_MODEL, vocab.total_answers)\n","\n","    def init_weights(self):\n","        for p in self.parameters():\n","            if p.dim() > 1:\n","                nn.init.xavier_uniform_(p)\n","\n","    def forward(self, questions: List[str], images: List[str]):\n","        images = [Image.open(image_path).convert(\"RGB\")  for image_path in images]\n","        vision_features,_ = self.vision_encoder(images)\n","        text_features,_ = self.text_embedding(questions)\n","\n","        fused_features = torch.cat([vision_features, text_features], dim=1)\n","        fused_features = self.dropout(self.fusion(fused_features))\n","        out = fused_features.sum(dim=1)\n","        out = self.proj(out)\n","\n","        return F.log_softmax(out, dim=-1)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"VFDXxPJ6ekrd","executionInfo":{"status":"ok","timestamp":1704127439979,"user_tz":-420,"elapsed":282,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[],"source":["class ClassificationTask():\n","    def __init__(self, config):\n","        # Create checkpoint folder if not existed\n","        self.checkpoint_path = os.path.join(BASE_DIR, config.TRAINING.CHECKPOINT_PATH, config.MODEL.NAME)\n","        if not os.path.isdir(self.checkpoint_path):\n","            print(\"Creating checkpoint path\")\n","            os.makedirs(self.checkpoint_path)\n","\n","        # Create/Load vocab\n","        if not os.path.isfile(os.path.join(self.checkpoint_path, \"vocab.bin\")):\n","            print(\"Creating vocab\")\n","            self.load_vocab(config.DATASET.VOCAB)\n","            print(\"Saving vocab to %s\" % os.path.join(self.checkpoint_path, \"vocab.bin\"))\n","            pickle.dump(self.vocab, open(os.path.join(self.checkpoint_path, \"vocab.bin\"), \"wb\"))\n","        else:\n","            print(\"Loading vocab from %s\" % os.path.join(self.checkpoint_path, \"vocab.bin\"))\n","            self.vocab = pickle.load(open(os.path.join(self.checkpoint_path, \"vocab.bin\"), \"rb\"))\n","\n","        print(\"Loading data\")\n","        self.load_dataset(config)\n","        self.create_dataloader(config)\n","\n","        self.config = config\n","        self.device = torch.device(config.MODEL.DEVICE)\n","        self.model = BEiTmBERTClassification(config.MODEL, self.vocab)\n","        self.model.to(self.device)\n","\n","        # Init hyperparameters\n","        self.epoch = 0\n","        self.score = config.TRAINING.SCORE\n","        self.score_value = 0.\n","        self.compute_score = ScoreCalculator()\n","        self.learning_rate = 1.0e-6 #config.TRAINING.LEARNING_RATE\n","        self.patience = config.TRAINING.PATIENCE\n","\n","        self.optim = Adam(self.model.parameters(), lr=self.learning_rate, betas=(0.9, 0.98))\n","        lambda_epoch = lambda epoch: 0.85 ** epoch\n","        self.scheduler = LambdaLR(self.optim, lr_lambda=lambda_epoch)\n","        self.loss_fn = NLLLoss(ignore_index=self.vocab.padding_idx)\n","\n","    def load_vocab(self, config):\n","        self.vocab = ClassificationVocab(config)\n","\n","    def load_dataset(self, config):\n","        train_df = pd.read_csv(config.DATASET.DF_PATH.TRAIN)\n","        X = train_df[['question', 'answer', 'img_id', 'type']]\n","        # Add a dummy target variable\n","        train_df['dummy_target'] = train_df['type']\n","        train_X, valid_X, train_dummy, valid_dummy = train_test_split(X, train_df['dummy_target'], test_size=0.2, random_state=42, stratify=train_df['dummy_target'])\n","        # Create dataframes for training and validation sets\n","        train_df = pd.DataFrame({'question': train_X['question'], 'answer': train_X['answer'], 'img_id': train_X['img_id'], 'type': train_X['type']})\n","        valid_df = pd.DataFrame({'question': valid_X['question'], 'answer': valid_X['answer'], 'img_id': valid_X['img_id'], 'type': valid_X['type']})\n","        train_df.reset_index(drop=True, inplace=True)\n","        valid_df.reset_index(drop=True, inplace=True)\n","        test_df = pd.read_csv(config.DATASET.DF_PATH.TEST)\n","        self.train_dataset = ViVQA_Dataset(train_df, config.DATASET.FEATURE_PATH.IMAGE, self.vocab)\n","        self.valid_dataset = ViVQA_Dataset(valid_df, config.DATASET.FEATURE_PATH.IMAGE, self.vocab)\n","        self.test_dataset = ViVQA_Dataset(test_df, config.DATASET.FEATURE_PATH.IMAGE, self.vocab)\n","        print(f'[INFO] Train size: {len(self.train_dataset)}')\n","        print(f'[INFO] Valid size: {len(self.valid_dataset)}')\n","        print(f'[INFO] Test size: {len(self.test_dataset)}')\n","\n","    def create_dataloader(self, config):\n","        self.train_dataloader = DataLoader(self.train_dataset, batch_size=config.DATASET.BATCH_SIZE, shuffle=True, num_workers=config.DATASET.WORKERS)\n","        self.valid_dataloader = DataLoader(self.valid_dataset, batch_size=config.DATASET.BATCH_SIZE, shuffle=False, num_workers=config.DATASET.WORKERS)\n","        self.test_dataloader = DataLoader(self.test_dataset, batch_size=config.DATASET.BATCH_SIZE, shuffle=False, num_workers=config.DATASET.WORKERS)\n","\n","    def load_checkpoint(self, fname) -> dict:\n","        if not os.path.exists(fname):\n","            return None\n","        print(f\"Loading checkpoint from {fname}\")\n","        checkpoint = torch.load(fname)\n","        self.score_value = checkpoint['score_value']\n","        self.model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n","        self.optim.load_state_dict(checkpoint['optimizer_state_dict'])\n","        return checkpoint\n","\n","    def save_checkpoint(self) -> None:\n","        dict_for_saving = {\n","            'epoch': self.epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optim.state_dict(),\n","            'score_value': self.score_value,\n","        }\n","        torch.save(dict_for_saving, os.path.join(self.checkpoint_path, \"last_model.pth\"))\n","\n","    def evaluate_loss(self, dataloader: DataLoader):\n","        self.model.eval()\n","        running_loss = .0\n","        with tqdm(desc='Epoch %d - Validation' % self.epoch, unit='it', total=len(dataloader)) as pbar:\n","            with torch.no_grad():\n","                for it, items in enumerate(dataloader):\n","                    quest, img, answer = items['question'], items['image'], items['answer']\n","                    with torch.no_grad():\n","                        out = self.model(quest, img).contiguous()\n","\n","                    answer = torch.stack([self.vocab.encode_answer(preprocess_sentence(ans, self.vocab.tokenizer)) for ans in answer]).to(self.device)\n","                    loss = self.loss_fn(out.view(-1, self.vocab.total_answers), answer.view(-1))\n","                    this_loss = loss.item()\n","                    running_loss += this_loss\n","\n","                    pbar.set_postfix(loss=running_loss / (it + 1))\n","                    pbar.update()\n","\n","        val_loss = running_loss / len(dataloader)\n","        return val_loss\n","\n","    def evaluate_metrics(self, dataloader: DataLoader):\n","        self.model.eval()\n","        wups=0.\n","        em=0.\n","        f1=0.\n","        cider=0.\n","        with tqdm(desc='Epoch %d - Evaluation' % self.epoch, unit='it', total=len(dataloader)) as pbar:\n","            for it, items in enumerate(dataloader):\n","                quest, img, answer = items['question'], items['image'], items['answer']\n","                with torch.no_grad():\n","                    outs = self.model(quest, img).contiguous()\n","\n","                answer = torch.stack([self.vocab.encode_answer(preprocess_sentence(ans, self.vocab.tokenizer)) for ans in answer]).to(self.device)\n","                answers_gt = self.vocab.decode_answer(answer.squeeze(-1), join_word=True)\n","                answers_gen = self.vocab.decode_answer(outs.argmax(dim=-1), join_word=True)\n","\n","                wups+=self.compute_score.wup(answers_gt, answers_gen)\n","                em+=self.compute_score.em(answers_gt, answers_gen)\n","                f1+=self.compute_score.f1_token(answers_gt, answers_gen)\n","                cider+=self.compute_score.cider_score(answers_gt, answers_gen)\n","                pbar.update()\n","\n","        scores ={\n","            'wups':wups/len(dataloader),\n","            'em':em/len(dataloader),\n","            'f1':f1/len(dataloader),\n","            'cider':cider/len(dataloader)\n","        }\n","        return scores\n","\n","    def train(self):\n","        self.model.train()\n","        running_loss = .0\n","        with tqdm(desc='Epoch %d - Training  ' % self.epoch, unit='it', total=len(self.train_dataloader)) as pbar:\n","            for it, items in enumerate(self.train_dataloader):\n","                quest, img, answer = items['question'], items['image'], items['answer']\n","                out = self.model(quest, img).contiguous()\n","                self.optim.zero_grad()\n","                answer = torch.stack([self.vocab.encode_answer(preprocess_sentence(ans, self.vocab.tokenizer)) for ans in answer]).to(self.device)\n","                loss = self.loss_fn(out.view(-1, self.vocab.total_answers), answer.view(-1))\n","                loss.backward()\n","\n","                self.optim.step()\n","                this_loss = loss.item()\n","                running_loss += this_loss\n","\n","                pbar.set_postfix({'loss': running_loss / (it + 1), 'lr':self.scheduler.get_last_lr()[0]})\n","                pbar.update()\n","        self.scheduler.step()\n","\n","        return running_loss / len(self.train_dataloader)\n","\n","    def start(self):\n","        if os.path.isfile(os.path.join(self.checkpoint_path, \"last_model.pth\")):\n","            checkpoint = self.load_checkpoint(os.path.join(self.checkpoint_path, \"last_model.pth\"))\n","            self.epoch = checkpoint[\"epoch\"] + 1\n","            print(\"Resuming from epoch %d\" % self.epoch)\n","            patience = 0\n","        else:\n","            self.score_value = .0\n","            patience = 0\n","\n","        while True:\n","            train_loss = self.train()\n","\n","            # val scores\n","            scores = self.evaluate_metrics(self.valid_dataloader)\n","            with open(os.path.join(self.checkpoint_path, \"log.txt\"), \"a\") as f:\n","                f.write(f\"Epoch {self.epoch:2d} - Training loss: {train_loss:.4f} - Validation wups: {scores['wups']:.4f} - Validation em: {scores['em']:.4f} - Validation f1: {scores['f1']:.4f} - Validation cider: {scores['cider']:.4f}\\n\")\n","\n","            print(f\"Validation wups: {scores['wups']:.4f} - em: {scores['em']:.4f} - f1: {scores['f1']:.4f} - cider: {scores['cider']:.4f}\")\n","            val_score = scores[self.score]\n","\n","            # Prepare for next epoch\n","            best = False\n","            if val_score > self.score_value:\n","                self.score_value = val_score\n","                patience = 0\n","                best = True\n","            else:\n","                patience += 1\n","\n","            exit_train = False\n","            if patience == self.patience:\n","                print('Patience reached.')\n","                exit_train = True\n","\n","            self.save_checkpoint()\n","\n","            if best:\n","                shutil.copy(os.path.join(self.checkpoint_path, \"last_model.pth\"),\n","                        os.path.join(self.checkpoint_path, \"best_model.pth\"))\n","\n","            if exit_train:\n","                break\n","\n","            self.epoch += 1\n","\n","    def get_predictions(self):\n","        if not os.path.isfile(os.path.join(self.checkpoint_path, 'best_model.pth')):\n","            print(\"Prediction require the model must be trained. There is no weights to load for model prediction!\")\n","            raise FileNotFoundError(\"Make sure your checkpoint path is correct or the best_model.pth is available in your checkpoint path\")\n","\n","        self.load_checkpoint(os.path.join(self.checkpoint_path, \"best_model.pth\"))\n","\n","        self.model.eval()\n","        img_path=[]\n","        quests_result=[]\n","        gts=[]\n","        preds=[]\n","        wups=0.\n","        em=0.\n","        f1=0.\n","        cider=0.\n","        with tqdm(desc='Getting predictions: ', unit='it', total=len(self.test_dataloader)) as pbar:\n","            for it, items in enumerate(self.test_dataloader):\n","                quest, img, answers = items['question'], items['image'], items['answer']\n","                with torch.no_grad():\n","                    outs = self.model(items['question'], items['image'])\n","\n","                answers_gen = self.vocab.decode_answer(outs.argmax(dim=-1), join_word=True)\n","\n","                img_path.extend(img)\n","                quests_result.extend(quest)\n","                gts.extend(answers)\n","                preds.extend(answers_gen)\n","                wups+=self.compute_score.wup(answers, answers_gen)\n","                em+=self.compute_score.em(answers, answers_gen)\n","                f1+=self.compute_score.f1_token(answers, answers_gen)\n","                cider+=self.compute_score.cider_score(answers, answers_gen)\n","\n","                pbar.update()\n","\n","        results={\n","            \"img_path\": img_path,\n","            \"question\": quests_result,\n","            \"ground_truth\":gts,\n","            \"predict\": preds,\n","        }\n","\n","        scores ={\n","            'wups':wups/len(self.test_dataloader),\n","            'em':em/len(self.test_dataloader),\n","            'f1':f1/len(self.test_dataloader),\n","            'cider':cider/len(self.test_dataloader)\n","        }\n","        print(f\"Evaluation scores on test - wups: {scores['wups']:.4f} - em: {scores['em']:.4f} - f1: {scores['f1']:.4f} - cider: {scores['cider']:.4f}\")\n","\n","        df = pd.DataFrame(results)\n","        df.to_csv(os.path.join(self.checkpoint_path,'result.csv'), index=False)\n","        print(f\"Save result to: {os.path.join(self.checkpoint_path,'result.csv')}\")"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1EyorZTPNbR","outputId":"8d8ebbcf-6f40-4b96-8a0c-24050a8ee105","executionInfo":{"status":"ok","timestamp":1704127451187,"user_tz":-420,"elapsed":10580,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading vocab from /content/drive/MyDrive/ViVQA-Models/beit_mbert_classification/vocab.bin\n","Loading data\n","[INFO] Train size: 9455\n","[INFO] Valid size: 2364\n","[INFO] Test size: 2987\n"]}],"source":["task = ClassificationTask(CONFIG)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r01z-tKceoYG","outputId":"fbf24fe3-5a08-47c6-fe4c-12c9f2292f57","executionInfo":{"status":"ok","timestamp":1704128862319,"user_tz":-420,"elapsed":1411167,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading checkpoint from /content/drive/MyDrive/ViVQA-Models/beit_mbert_classification/last_model.pth\n","Resuming from epoch 43\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 43 - Training  : 100%|██████████| 296/296 [03:41<00:00,  1.33it/s, loss=0.843, lr=1e-6]\n","Epoch 43 - Evaluation: 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation wups: 0.4558 - em: 0.4558 - f1: 0.5161 - cider: 2.0828\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 44 - Training  : 100%|██████████| 296/296 [03:39<00:00,  1.35it/s, loss=0.692, lr=8.5e-7]\n","Epoch 44 - Evaluation: 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation wups: 0.4548 - em: 0.4548 - f1: 0.5154 - cider: 2.0837\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 45 - Training  : 100%|██████████| 296/296 [03:37<00:00,  1.36it/s, loss=0.754, lr=7.22e-7]\n","Epoch 45 - Evaluation: 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation wups: 0.4579 - em: 0.4579 - f1: 0.5169 - cider: 2.0944\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 46 - Training  : 100%|██████████| 296/296 [03:38<00:00,  1.36it/s, loss=0.686, lr=6.14e-7]\n","Epoch 46 - Evaluation: 100%|██████████| 74/74 [00:53<00:00,  1.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation wups: 0.4578 - em: 0.4578 - f1: 0.5172 - cider: 2.0938\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 47 - Training  : 100%|██████████| 296/296 [03:38<00:00,  1.35it/s, loss=0.686, lr=5.22e-7]\n","Epoch 47 - Evaluation: 100%|██████████| 74/74 [00:53<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Validation wups: 0.4565 - em: 0.4565 - f1: 0.5166 - cider: 2.0886\n","Patience reached.\n"]}],"source":["task.start()"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FY9add1leraI","outputId":"8303a91a-c6a8-4849-8dad-1c6be0f59e85","executionInfo":{"status":"ok","timestamp":1704128941199,"user_tz":-420,"elapsed":78896,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading checkpoint from /content/drive/MyDrive/ViVQA-Models/beit_mbert_classification/best_model.pth\n"]},{"output_type":"stream","name":"stderr","text":["Getting predictions: 100%|██████████| 94/94 [01:12<00:00,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":["Evaluation scores on test - wups: 0.4607 - em: 0.4607 - f1: 0.5031 - cider: 2.0601\n","Save result to: /content/drive/MyDrive/ViVQA-Models/beit_mbert_classification/result.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["task.get_predictions()"]},{"cell_type":"code","source":["!pip install -q torchinfo"],"metadata":{"id":"dBi_q61-5-c7","executionInfo":{"status":"ok","timestamp":1704128956310,"user_tz":-420,"elapsed":8255,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","execution_count":33,"metadata":{"id":"GDr3ly85PG8P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704128983519,"user_tz":-420,"elapsed":738,"user":{"displayName":"Hiếu Nguyễn","userId":"03621851764524954359"}},"outputId":"884a0a66-a6e9-4c9c-a346-68eb034b2bae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["====================================================================================================\n","Layer (type:depth-idx)                                                      Param #\n","====================================================================================================\n","BEiTmBERTClassification                                                     --\n","├─BertEmbedding: 1-1                                                        --\n","│    └─TextBert: 2-1                                                        --\n","│    │    └─BertEmbeddings: 3-1                                             (81,711,360)\n","│    │    └─BertEncoder: 3-2                                                (85,054,464)\n","│    └─Linear: 2-2                                                          393,728\n","│    └─GELU: 2-3                                                            --\n","│    └─Dropout: 2-4                                                         --\n","├─BEiTEmbedding: 1-2                                                        --\n","│    └─BeitModel: 2-5                                                       --\n","│    │    └─BeitEmbeddings: 3-3                                             (591,360)\n","│    │    └─BeitEncoder: 3-4                                                (85,169,088)\n","│    │    └─Identity: 3-5                                                   --\n","│    │    └─BeitPooler: 3-6                                                 (1,536)\n","│    └─Linear: 2-6                                                          393,728\n","│    └─GELU: 2-7                                                            --\n","│    └─Dropout: 2-8                                                         --\n","├─Linear: 1-3                                                               262,656\n","├─Dropout: 1-4                                                              --\n","├─LayerNorm: 1-5                                                            1,024\n","├─Linear: 1-6                                                               180,063\n","====================================================================================================\n","Total params: 253,759,007\n","Trainable params: 1,231,199\n","Non-trainable params: 252,527,808\n","===================================================================================================="]},"metadata":{},"execution_count":33}],"source":["from torchinfo import summary\n","summary(task.model)"]},{"cell_type":"code","source":[],"metadata":{"id":"LE2mDVt9AKwk"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}